{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22dc2c8e",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "We will preprocess each raw dataset into a single CSV folder with two columns: \"instructions\" and \"Output\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e3652c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned data saved to 'C:\\Users\\jayac\\OneDrive\\Desktop\\MedicBot\\MedicBot\\Dataset\\Data_Preprocessed.csv'\n"
     ]
    }
   ],
   "source": [
    "''' For the first dataset , It contains Description, Patient, Doctor columns where it has values as questionare and answers. We will remove the Description column and remove the words \n",
    "\"Hi Doctor\" and \"Hello Doctor\" with just simple \"Hello\" or \"Hi\".There were other letters such as √Ç, we need to remove that also.\n",
    "Then Create a new csv for our integrated data and store it accordingly.'''\n",
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "\n",
    "INPUT_CSV_PATH = r\"MedicBot\\raw_dataset\\ai-medical-chatbot.csv\"        \n",
    "OUTPUT_CSV_PATH = r\"MedicBot\\MedicBot\\Dataset\\Data_Preprocessed.csv\"   # Output file\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(INPUT_CSV_PATH)\n",
    "except Exception as e:\n",
    "    print(f\"Error loading CSV: {e}\")\n",
    "    exit(1)\n",
    "\n",
    "if 'Description' in df.columns:\n",
    "    df.drop(columns=['Description'], inplace=True) # Droping Description\n",
    "\n",
    "def clean_greeting(text):\n",
    "    # Remove encoding artifacts\n",
    "    text = re.sub(r\"[√ÇÔøΩ]\", \"\", text)\n",
    "    # Replace variations of 'Hi Doctor' or 'Hello Doctor' (case insensitive)\n",
    "    text = re.sub(r\"\\bHi\\s+Doctor\\b\", \"Hi\", text, flags=re.IGNORECASE)\n",
    "    text = re.sub(r\"\\bHello\\s+Doctor\\b\", \"Hello\", text, flags=re.IGNORECASE)\n",
    "    return text\n",
    "\n",
    "if 'Patient' in df.columns:\n",
    "    df['Patient'] = df['Patient'].astype(str).apply(clean_greeting)\n",
    "else:\n",
    "    print(\"Error: 'Patient' column not found.\")\n",
    "    exit(1)\n",
    "\n",
    "df.to_csv(OUTPUT_CSV_PATH, index=False)\n",
    "print(f\"Cleaned data saved to '{OUTPUT_CSV_PATH}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f205a295",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully appended 11901 rows to 'C:\\Users\\jayac\\OneDrive\\Desktop\\MedicBot\\MedicBot\\Dataset\\Data_Preprocessed.csv'\n"
     ]
    }
   ],
   "source": [
    "''' For the next dataset, we will choose the first and second column and add those to the preprocessed dataset.Same for other files also'''\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Paths to your files\n",
    "OUTPUT_CSV_PATH = r\"MedicBot\\MedicBot\\Dataset\\Data_Preprocessed.csv\"   # Output file\n",
    "DATASET_CSV = r\"MedicBot\\MedicBot\\raw_dataset\\validation_data_chatbot.csv\"  # Update this with actual path if needed\n",
    "\n",
    "# Load main preprocessed dataset\n",
    "df_main = pd.read_csv(OUTPUT_CSV_PATH)\n",
    "\n",
    "# Load second dataset with specified columns\n",
    "df_new = pd.read_csv(DATASET_CSV, usecols=[\"short_question\", \"short_answer\"])\n",
    "\n",
    "# Rename columns to match the format\n",
    "df_new.columns = ['Patient', 'Doctor']\n",
    "\n",
    "# Append to the existing dataset\n",
    "df_main = pd.concat([df_main, df_new], ignore_index=True)\n",
    "\n",
    "# Save back to same file\n",
    "df_main.to_csv(OUTPUT_CSV_PATH, index=False, encoding='utf-8')\n",
    "\n",
    "print(f\"Successfully appended {len(df_new)} rows to '{OUTPUT_CSV_PATH}'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b335d17f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data successfully appended from JSON to CSV.\n"
     ]
    }
   ],
   "source": [
    "''' For the next dataset, it is a json file, we will load the json file and in the json file we will choose the random question from the patterns section\n",
    "and add that to the preprocessed dataset. We will also choose the answer from the responses section and add that to the preprocessed dataset.'''\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import random\n",
    "import pandas as pd\n",
    "\n",
    "# Path to your existing preprocessed CSV\n",
    "OUTPUT_CSV_PATH = r\"MedicBot\\Dataset\\Data_Preprocessed.csv\"   # Output file\n",
    "\n",
    "# Path to your new JSON file\n",
    "json_file = r\"MedicBot\\MedicBot\\raw_dataset\\intents.json\"\n",
    "\n",
    "# Load the existing CSV into a DataFrame\n",
    "df = pd.read_csv(OUTPUT_CSV_PATH)\n",
    "\n",
    "# Load the JSON dataset\n",
    "with open(json_file, 'r', encoding='utf-8') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Prepare a list of new rows\n",
    "new_rows = []\n",
    "# Extract and append random pattern-response pairs\n",
    "for intent in data[\"intents\"]:\n",
    "    if intent[\"patterns\"] and intent[\"responses\"]:\n",
    "        question = random.choice(intent[\"patterns\"]).strip()\n",
    "        answer = intent[\"responses\"][0].strip()\n",
    "        new_rows.append({\"Patient\": question, \"Doctor\": answer})\n",
    "\n",
    "# Convert to DataFrame and concatenate\n",
    "new_df = pd.DataFrame(new_rows)\n",
    "df = pd.concat([df, new_df], ignore_index=True)\n",
    "\n",
    "# Save updated CSV (overwriting)\n",
    "df.to_csv(OUTPUT_CSV_PATH, index=False, encoding='utf-8')\n",
    "print(\"Data successfully appended from JSON to CSV.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08b334e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Dataset Overview\n",
      "----------------------------------------\n",
      "Shape (rows, columns): (318464, 2)\n",
      "Columns: ['Patient', 'Doctor']\n",
      "\n",
      "Column Types:\n",
      "Patient    object\n",
      "Doctor     object\n",
      "dtype: object\n",
      "\n",
      "üî¢ Unique Value Counts:\n",
      "Patient: 269488 unique values\n",
      "Doctor: 272191 unique values\n",
      "\n",
      "üßæ First 5 Rows:\n",
      "                                             Patient  \\\n",
      "0  Hi,I am just wondering what is abutting and ab...   \n",
      "1  Hi, I am a 22-year-old female who was diagnose...   \n",
      "2  Hi! I used to have clear skin but since I move...   \n",
      "3  Hello,I am having an uncomfortable feeling in ...   \n",
      "4  Hello,Before two years had sex with a call gir...   \n",
      "\n",
      "                                              Doctor  \n",
      "0  Hi. I have gone through your query with dilige...  \n",
      "1  Hi. You have really done well with the hypothy...  \n",
      "2  Hi there Acne has multifactorial etiology. Onl...  \n",
      "3  Hello. The popping and discomfort what you fel...  \n",
      "4  Hello. The HIV test uses a finger prick blood ...  \n",
      "\n",
      "‚ùì Missing Values:\n",
      "Patient    0\n",
      "Doctor     0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Try a more tolerant encoding\n",
    "csv_path = r\"MedicBot\\MedicBot\\Dataset\\Data_Preprocessed.csv\"\n",
    "df = pd.read_csv(csv_path, encoding='ISO-8859-1')  # Or try 'cp1252' if needed\n",
    "\n",
    "# Dataset summary\n",
    "print(\"üîç Dataset Overview\")\n",
    "print(\"-\" * 40)\n",
    "print(f\"Shape (rows, columns): {df.shape}\")\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(\"\\nColumn Types:\")\n",
    "print(df.dtypes)\n",
    "\n",
    "# Unique values in each column\n",
    "print(\"\\nüî¢ Unique Value Counts:\")\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].nunique()} unique values\")\n",
    "\n",
    "# Show sample rows\n",
    "print(\"\\nüßæ First 5 Rows:\")\n",
    "print(df.head())\n",
    "\n",
    "# Check for nulls\n",
    "print(\"\\n‚ùì Missing Values:\")\n",
    "print(df.isnull().sum())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "271f015e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Duplicates based on 'Doctor' column removed.\n",
      "Remaining rows: 272191\n",
      "Columns: ['Patient', 'Doctor']\n"
     ]
    }
   ],
   "source": [
    "# Data Cleaning - Remove duplicates\n",
    "import pandas as pd\n",
    "\n",
    "# Load the dataset with tolerant encoding\n",
    "csv_path = r\"MedicBot\\MedicBot\\Dataset\\Data_Preprocessed.csv\"\n",
    "df = pd.read_csv(csv_path, encoding='ISO-8859-1')\n",
    "\n",
    "# Drop duplicate Doctor responses, keeping the first occurrence\n",
    "df_unique = df.drop_duplicates(subset='Doctor', keep='first')\n",
    "\n",
    "# Save the cleaned DataFrame back to the same file (overwrite)\n",
    "df_unique.to_csv(csv_path, index=False, encoding='utf-8')\n",
    "\n",
    "# Summary after cleaning\n",
    "print(\"‚úÖ Duplicates based on 'Doctor' column removed.\")\n",
    "print(f\"Remaining rows: {df_unique.shape[0]}\")\n",
    "print(f\"Columns: {df_unique.columns.tolist()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a3f5f03c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 272191/272191 [00:46<00:00, 5840.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Conversion complete! Saved 272191 conversations to medical_chatbot_llama3_format.json\n"
     ]
    }
   ],
   "source": [
    "#converting the dataset to LLaMA 3 format in a json file\n",
    "import pandas as pd\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Load your CSV data\n",
    "df = pd.read_csv(r'C:\\Users\\jayac\\OneDrive\\Desktop\\MedicBot\\MedicBot\\Dataset\\Data_Preprocessed.csv') \n",
    "\n",
    "# Initialize conversation list\n",
    "conversations = []\n",
    "\n",
    "# Template for system prompt\n",
    "system_prompt = {\n",
    "    \"role\": \"system\",\n",
    "    \"content\": \"You are a helpful medical AI assistant specialized in providing accurate medical information and advice.\"\n",
    "}\n",
    "\n",
    "# Process each row in the CSV\n",
    "for _, row in tqdm(df.iterrows(), total=len(df)):\n",
    "    question = row['Patient']  \n",
    "    response = row['Doctor'] \n",
    "    \n",
    "    # Create conversation dictionary\n",
    "    conversation = {\n",
    "        \"messages\": [\n",
    "            system_prompt,\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": f\"<|start_header_id|>user<|end_header_id|>\\n\\n{question}<|eot_id|>\"\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": f\"<|start_header_id|>assistant<|end_header_id|>\\n\\n{response}<|eot_id|>\"\n",
    "            }\n",
    "        ]\n",
    "    }\n",
    "    conversations.append(conversation)\n",
    "\n",
    "# Save to JSON file\n",
    "with open('medical_dataset_llama3_format.json', 'w') as f:\n",
    "    json.dump(conversations, f, indent=2)\n",
    "\n",
    "print(f\"Conversion complete! Saved {len(conversations)} conversations to medical_chatbot_llama3_format.json\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
