# -*- coding: utf-8 -*-
"""main.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1LquPMXL3UnqWQWn3ndbPoZKuKJFY0T3A
"""

# ✅ Step 1: Install dependencies
!pip install -q transformers datasets accelerate

# ✅ Step 2: Check GPU availability
import torch
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print("Using device:", device)

from huggingface_hub import login

# Paste your token between the quotes
login("Your Access Token")

# ✅ Step 3: Load tokenizer and model
from transformers import AutoTokenizer, AutoModelForCausalLM

model_name = "meta-llama/Llama-3.2-1B"

tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)
tokenizer.pad_token = tokenizer.eos_token  # LLaMA needs a pad token

model = AutoModelForCausalLM.from_pretrained(
    model_name,
    torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,
    device_map="auto"
)

!rm -rf ~/.cache/huggingface/datasets

import json
from datasets import Dataset

# Load JSON file manually
with open("/content/medical_dataset_llama3_format.json") as f:
    try:
        data=json.load(f)
        print("JSON is valid!")
    except json.JSONDecodeError as e:
        print(f"Invalid JSON: {e}")


# Convert to Hugging Face Dataset
dataset = Dataset.from_list(data)

# Train/test split
dataset = dataset.train_test_split(test_size=0.2)

# View a sample
print(dataset["train"][0])

def safe_convert_to_messages(data):
    """Safely convert message data to proper format"""
    if isinstance(data, str):
        try:
            return json.loads(data.replace("'", '"'))
        except json.JSONDecodeError:
            return [{"role": "user", "content": data}]
    elif isinstance(data, list):
        return data
    return [{"role": "user", "content": str(data)}]

def format_conversation(messages):
    """Convert messages to Llama 3 chat format"""
    formatted = []
    for msg in messages:
        if not isinstance(msg, dict):
            msg = {"role": "user", "content": str(msg)}
        content = msg.get("content", "")
        # Clean special tokens if present
        if "<|start_header_id|>" in content:
            content = content.split("\n\n")[-1].replace("<|eot_id|>", "")
        formatted.append({"role": msg.get("role", "user"), "content": content})
    return formatted

def tokenize_function(batch):
    """Process batch of examples"""
    formatted_texts = []
    for messages in batch["messages"]:
        # Convert to proper message format
        messages = safe_convert_to_messages(messages)
        messages = format_conversation(messages)

        # Apply chat template
        try:
            formatted = tokenizer.apply_chat_template(
                messages,
                tokenize=False,
                add_generation_prompt=False
            )
            formatted_texts.append(formatted)
        except Exception as e:
            print(f"Skipping malformed message: {e}")
            formatted_texts.append("")  # Empty string for padding

    # Tokenize all texts
    tokenized = tokenizer(
        formatted_texts,
        truncation=True,
        padding="max_length",
        max_length=1024,  # Reduced for safety
        return_tensors="pt"
    )
    tokenized["labels"] = tokenized["input_ids"].clone()
    return tokenized

# Apply tokenization
tokenized_dataset = dataset.map(
    tokenize_function,
    batched=True,
    batch_size=4,
    remove_columns=["messages"]
)

# Set format for training
tokenized_dataset.set_format(
    type="torch",
    columns=["input_ids", "attention_mask", "labels"]
)

# ✅ Step 5: Tokenize
def tokenize_function(examples):
    return tokenizer(examples["text"], truncation=True, padding="max_length", max_length=128)

# Apply tokenizer to both train and test sets
tokenized_dataset = dataset.map(tokenize_function, batched=True)

# Format for PyTorch
tokenized_dataset.set_format(type="torch", columns=["input_ids", "attention_mask"])

from torch.utils.data import DataLoader

# Correct way to split the dataset (using the dataset's method)
split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=42)

# Then create DataLoaders
train_dataloader = DataLoader(
    split_dataset["train"],
    batch_size=2,
    shuffle=True
)

eval_dataloader = DataLoader(
    split_dataset["test"],
    batch_size=2
)

print(f"Train batches: {len(train_dataloader)}")
print(f"Eval batches: {len(eval_dataloader)}")

from torch.optim import AdamW

# Initialize optimizer with model parameters and learning rate
optimizer = AdamW(model.parameters(), lr=5e-5, weight_decay=0.01)

!pip install numpy==1.26.4
import os
os._exit(00)  # Restart the runtime after install

from tqdm import tqdm
import torch

model.train()
model.to(device)

epochs = 1

for epoch in range(epochs):
    loop = tqdm(train_dataloader, leave=True)

    for batch in loop:
        # Move inputs to device
        input_ids = batch["input_ids"].to(device)
        attention_mask = batch["attention_mask"].to(device)

        # Forward pass with labels = input_ids for causal language modeling
        outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=input_ids)
        loss = outputs.loss

        # Backpropagation
        loss.backward()
        optimizer.step()
        optimizer.zero_grad()

        # Logging
        loop.set_description(f"Epoch {epoch + 1}")
        loop.set_postfix(loss=loss.item())

# ✅ Step 9: Save model and tokenizer
model.save_pretrained("./llama3-1b-finetuned-manual")
tokenizer.save_pretrained("./llama3-1b-finetuned-manual")